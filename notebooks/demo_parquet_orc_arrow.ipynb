{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding, Decoding and Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Parquet, ORC and Arrow\n",
    "\n",
    "We can easily read (decode) and write (encode) data from and to Parquet, ORC and Arrow files interchangeably. The `pyarrow` library allows us to read a Parquet or ORC file into a `pyarrow.Table` object, which is a columnar data structure that can be converted to a Pandas DataFrame. We can also write a `pyarrow.Table` to a Parquet or ORC file.\n",
    "\n",
    "Parquet has the following types:\n",
    "\n",
    "- boolean: 1 bit boolean\n",
    "- int32: 32 bit signed ints\n",
    "- int64: 64 bit signed ints\n",
    "- int96: 96 bit signed ints\n",
    "- float: IEEE 32-bit floating point values\n",
    "- double: IEEE 64-bit floating point values\n",
    "- byte_array: arbitrarily long byte arrays\n",
    "- fixed_len_byte_array: fixed length byte arrays\n",
    "- string: UTF-8 encoded strings\n",
    "- enum: enumeration of strings\n",
    "- temporal: a logical date type\n",
    "\n",
    "ORC has the following types:\n",
    "\n",
    "- boolean: 1 bit boolean\n",
    "- tinyint: 8 bit signed ints\n",
    "- smallint: 16 bit signed ints\n",
    "- int: 32 bit signed ints\n",
    "- bigint: 64 bit signed ints\n",
    "- float: IEEE 32-bit floating point values\n",
    "- double: IEEE 64-bit floating point values\n",
    "- string: UTF-8 encoded strings\n",
    "- char: ASCII strings\n",
    "- varchar: UTF-8 strings\n",
    "- binary: byte arrays\n",
    "- timestamp: a logical date type\n",
    "- date: a logical date type\n",
    "- decimal: arbitrary precision decimals\n",
    "- list: an ordered collection of objects\n",
    "- map: a collection of key-value pairs\n",
    "- struct: an ordered collection of named fields\n",
    "- union: a list of types\n",
    "\n",
    "![overview-diagram](../assets/diagram-2.png)\n",
    "\n",
    "### Reading (Decoding) and Writing (Encoding) a Parquet File\n",
    "\n",
    "Let's look at how to decode and encode a Parquet file with mock customers data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table('../data/userdata1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pq.read_metadata('../data/userdata1.parquet')\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.row_group(0).column(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the first 3 rows of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.take([0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a Table to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert the DataFrame back to a Table (note we're using the method from `pa` which is pyarrow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table = pa.Table.from_pandas(df)\n",
    "\n",
    "new_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write the table back to a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(new_table, \"../data/userdata2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. How many males and females are there?\n",
    ">\n",
    "> 2. What is the average salary for customers from China?\n",
    ">\n",
    "> 3. Create a new column `full_name` which combines `first_name` and `last_name` with a space in between in the dataframe. Then convert it back to a new Table and write it to a Parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading (Decoding) and Writing (Encoding) an ORC File\n",
    "\n",
    "Let's look at how to decode and encode an ORC file with mock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from pyarrow import orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = orc.read_table('../data/userdata1.1.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = table2.to_pandas()\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write the table back to an ORC file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orc.write_table(table2, \"../data/file2.orc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
